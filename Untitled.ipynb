{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6996706c-7639-4dc7-993b-512c67297935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# 오늘기준으로 1년치 주가 csv 파일로 로컬에 저장\n",
    "\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "datetime.now().date()\n",
    "df = yf.download('005930.KS',datetime.now().date()-timedelta(365),datetime.now().date())\n",
    "df.to_csv('삼성전자.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1080f725-e6bb-4c26-8d41-4bede9b934de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2022, 8, 26)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from datetime import datetime, timedelta\n",
    "datetime.now().date()-timedelta(365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d406902-a42b-4c6e-9724-0a3b02142279",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86ee47e7-c4b2-4d36-92af-cc65402023fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import (\n",
    "    functions as f,\n",
    "    Row,\n",
    "    SparkSession,\n",
    "    types as t\n",
    ")\n",
    "\n",
    "from pyspark.sql.functions import col, asc, desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "036c2b28-893f-4a42-b023-9450111f0588",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create SparkSession \n",
    "spark = SparkSession.builder \\\n",
    "      .master(\"local[1]\") \\\n",
    "      .appName(\"SparkByExamples.com\") \\\n",
    "      .getOrCreate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a124f12-0694-4e36-84bf-391ff81680fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = \"삼성전자.csv\"\n",
    "df = spark.read.option(\"header\",\"true\") \\\n",
    "    .option(\"inferSchema\",\"true\") \\\n",
    "    .csv(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d42a4155-15bd-4227-b0c2-c3f5687a2daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+-------+-------+--------------+--------+\n",
      "|      Date|   Open|   High|    Low|  Close|     Adj Close|  Volume|\n",
      "+----------+-------+-------+-------+-------+--------------+--------+\n",
      "|2023-01-02|55500.0|56100.0|55200.0|55500.0|  54906.453125|10031448|\n",
      "|2023-01-03|55400.0|56000.0|54500.0|55400.0|54807.51953125|13547030|\n",
      "|2023-01-04|55700.0|58000.0|55600.0|57800.0| 57181.8515625|20188071|\n",
      "|2023-01-05|58200.0|58800.0|57600.0|58200.0|57577.57421875|15682826|\n",
      "|2023-01-06|58300.0|59400.0|57900.0|59000.0|58369.01953125|17334989|\n",
      "|2023-01-09|59700.0|60700.0|59600.0|60700.0| 60050.8359375|18640107|\n",
      "|2023-01-10|60200.0|61100.0|59900.0|60400.0|  59754.046875|14859797|\n",
      "|2023-01-11|61000.0|61200.0|60300.0|60500.0|59852.98046875|12310751|\n",
      "|2023-01-12|61100.0|61200.0|59900.0|60500.0|59852.98046875|16102561|\n",
      "|2023-01-13|60500.0|61200.0|60400.0|60800.0|60149.76953125|12510328|\n",
      "|2023-01-16|61300.0|61600.0|60800.0|61100.0|    60446.5625|10039972|\n",
      "|2023-01-17|61200.0|61500.0|60600.0|61000.0|60347.62890625| 9831456|\n",
      "|2023-01-18|60700.0|61000.0|59900.0|60400.0|  59754.046875|11584041|\n",
      "|2023-01-19|60500.0|61500.0|60400.0|61500.0|60842.28515625|12808490|\n",
      "|2023-01-20|62100.0|62300.0|61100.0|61800.0|61139.07421875| 9646327|\n",
      "|2023-01-25|63500.0|63700.0|63000.0|63300.0|   62623.03125|   39217|\n",
      "|2023-01-26|63800.0|63900.0|63300.0|63800.0|    63117.6875|   54188|\n",
      "|2023-01-27|64400.0|65000.0|63900.0|64600.0|63909.12890625|   44282|\n",
      "|2023-01-30|64900.0|64900.0|63100.0|63300.0|   62623.03125|   59628|\n",
      "|2023-01-31|63500.0|63700.0|61000.0|61400.0|60743.35546875|  163847|\n",
      "+----------+-------+-------+-------+-------+--------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = df.filter(df.Date>='2023-01-01')\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc2e5efc-86c1-4493-94ea-5a980f38840d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+\n",
      "|      Date|  Close| expect|\n",
      "+----------+-------+-------+\n",
      "|2023-01-02|55500.0|55500.0|\n",
      "|2023-01-03|55400.0|55400.0|\n",
      "|2023-01-04|57800.0|57800.0|\n",
      "|2023-01-05|58200.0|58200.0|\n",
      "|2023-01-06|59000.0|59000.0|\n",
      "|2023-01-09|60700.0|60700.0|\n",
      "|2023-01-10|60400.0|60400.0|\n",
      "|2023-01-11|60500.0|60500.0|\n",
      "|2023-01-12|60500.0|60500.0|\n",
      "|2023-01-13|60800.0|60800.0|\n",
      "|2023-01-16|61100.0|61100.0|\n",
      "|2023-01-17|61000.0|61000.0|\n",
      "|2023-01-18|60400.0|60400.0|\n",
      "|2023-01-19|61500.0|61500.0|\n",
      "|2023-01-20|61800.0|61800.0|\n",
      "|2023-01-25|63300.0|63300.0|\n",
      "|2023-01-26|63800.0|63800.0|\n",
      "|2023-01-27|64600.0|64600.0|\n",
      "|2023-01-30|63300.0|63300.0|\n",
      "|2023-01-31|61400.0|61400.0|\n",
      "+----------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "expect_data = data.withColumn(\"expect\",f.col(\"Close\")).select(\"Date\",\"Close\",\"expect\")\n",
    "    \n",
    "expect_data.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "050b725a-9f84-4957-baed-da3505bc34fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+\n",
      "|      Date|  Close| expect|\n",
      "+----------+-------+-------+\n",
      "|2023-01-02|55500.0|55500.0|\n",
      "|2023-01-03|55400.0|55400.0|\n",
      "|2023-01-04|57800.0|57800.0|\n",
      "|2023-01-05|58200.0|58200.0|\n",
      "|2023-01-06|59000.0|59000.0|\n",
      "|2023-01-09|60700.0|60700.0|\n",
      "|2023-01-10|60400.0|60400.0|\n",
      "|2023-01-11|60500.0|60500.0|\n",
      "|2023-01-12|60500.0|60500.0|\n",
      "|2023-01-13|60800.0|60800.0|\n",
      "|2023-01-16|61100.0|61100.0|\n",
      "|2023-01-17|61000.0|61000.0|\n",
      "|2023-01-18|60400.0|60400.0|\n",
      "|2023-01-19|61500.0|61500.0|\n",
      "|2023-01-20|61800.0|61800.0|\n",
      "|2023-01-25|63300.0|63300.0|\n",
      "|2023-01-26|63800.0|63800.0|\n",
      "|2023-01-27|64600.0|64600.0|\n",
      "|2023-01-30|63300.0|63300.0|\n",
      "|2023-01-31|61400.0|61400.0|\n",
      "+----------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # ROW 추가\n",
    "# columns = [\"Date\", \"Close\", \"expect\"]\n",
    "\n",
    "# new_row = spark.createDataFrame([('2023-08-25', 68200.0, 68200.0)], columns)\n",
    "# expect_data = expect_data.union(new_row)\n",
    "# expect_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c17d43-a19b-4c7f-8307-a104aad1a09f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c57d5697-3d33-47da-aca6-5ba388f37041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236\n",
      "+-------------------+\n",
      "|date_add(Date, 236)|\n",
      "+-------------------+\n",
      "|         2023-08-26|\n",
      "|         2023-08-27|\n",
      "|         2023-08-28|\n",
      "|         2023-08-29|\n",
      "|         2023-08-30|\n",
      "|         2023-09-02|\n",
      "|         2023-09-03|\n",
      "|         2023-09-04|\n",
      "|         2023-09-05|\n",
      "|         2023-09-06|\n",
      "|         2023-09-09|\n",
      "|         2023-09-10|\n",
      "|         2023-09-11|\n",
      "|         2023-09-12|\n",
      "|         2023-09-13|\n",
      "|         2023-09-18|\n",
      "|         2023-09-19|\n",
      "|         2023-09-20|\n",
      "|         2023-09-23|\n",
      "|         2023-09-24|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 마지막 날짜를 일수로 변환\n",
    "from pyspark.sql.functions import date_add, date_sub, col\n",
    "\n",
    "today = expect_data.select(f.dayofyear('Date')).tail(1)[0][0]\n",
    "print(today)\n",
    "# f.dayofyear(expect_data.tail(1)[0][0]).show()\n",
    "# f.dayofyear(expect_data.tail(1)[0][0]).show()\n",
    "\n",
    "# lastday = int(expect_data.tail(1)[0][0].split(\"-\")[2])\n",
    "# print(lastday+1)\n",
    "\n",
    "# dateDF.select(date_sub(col(\"today\"), 5), date_add(col(\"today\"), 5)).show(1)\n",
    "expect_data.select(date_add(col(\"Date\"),today)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "52f8c8e8-aa09-44b8-ac9c-f8d91953872d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 날짜 :  2023-08-27\n",
      "           Date\n",
      "0    2023-08-26\n",
      "1    2023-08-27\n",
      "2    2023-08-28\n",
      "3    2023-08-29\n",
      "4    2023-08-30\n",
      "..          ...\n",
      "175  2024-02-17\n",
      "176  2024-02-18\n",
      "177  2024-02-19\n",
      "178  2024-02-20\n",
      "179  2024-02-21\n",
      "\n",
      "[180 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# 180 기간동안의 예측 데이터\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import current_date, current_timestamp\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "now = datetime.now()\n",
    "\n",
    "print(\"현재 날짜 : \", datetime.now().date()+timedelta(days=1))\n",
    "\n",
    "datelist = []\n",
    "\n",
    "for x in range(180) :\n",
    "    datelist.append(datetime.now().date()+timedelta(days=x))\n",
    "df = pd.DataFrame({'Date': datelist})\n",
    "print(df)\n",
    "# print(datelist)\n",
    "\n",
    "# afterDF = spark.createDataFrame(datelist,'date')\n",
    "\n",
    "# afterDF.rename(columns={'value':'Date'})\n",
    "# afterDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faac62bd-ff06-4dea-92e8-38af067d21fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
